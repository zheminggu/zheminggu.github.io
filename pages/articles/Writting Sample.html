<!DOCTYPE html>
<!--[if IE 8 ]><html class="no-js oldie ie8" lang="en"> <![endif]-->
<!--[if IE 9 ]><html class="no-js oldie ie9" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!--><html class="no-js" lang="en"> <!--<![endif]-->
<head>

   <!--- basic page needs
   ================================================== -->
   <meta charset="utf-8">
	<title>Gu's Writting Sample</title>
	<meta name="description" content="">  
	<meta name="author" content="">

   <!-- mobile specific metas
   ================================================== -->
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

 	<!-- CSS
   ================================================== -->
   <link rel="stylesheet" href="../../css/base.css"> 
   <link rel="stylesheet" href="../../css/vendor.css"> 
   <link rel="stylesheet" href="../../css/main.css"> 

   <style type="text/css" media="screen">
   	#styles { 
   		background: rgb(255, 255, 255);
   		padding-top: 12rem;
		padding-bottom: 12rem;


   	}      	
   </style>    

   <!-- script
   ================================================== -->
	<script src="../../js/modernizr.js"></script>
	<script src="../../js/pace.min.js"></script>

   <!-- favicons
	================================================== -->
	<link rel="icon" type="image/png" href="../../favicon.png">

</head>

<body id="top" >
	
	<!-- header 
   ================================================== -->
   <header class="main-header">
   	
   	<div class="logo">
	      <a href="../../index.html">Home</a>
	   </div> 

	   <a class="menu-toggle" href="#"><span>Menu</span></a>   	

   </header>

   <!-- main navigation 
   ================================================== -->
   <nav id="menu-nav-wrap">
		<h3>Navigation</h3>   	
		<ul class="nav-list">
			<li><a href="../../index.html" title="">Home</a></li>
			<li><a href="#" title="">Statement of purpose</a></li>
			<li><a href="Writting Sample.html" title="">Writting Sample</a></li>
			<li><a href="Video Speech.html" title="">Video Speech</a></li>
		</ul>
	</nav> <!-- /menu-nav-wrap -->


	<!-- main content wrap
   ================================================== -->
   <div id="main-content-wrap">


		<!-- main content wrap
   	================================================== -->
   	<section id="intro">

		   <!-- <div class="shadow-overlay"></div> -->
		   
		   <div class="row intro-content">
		   	<div class="col-twelve">

		   		<h3 class="animate-intro">Zheming Gu</h3>
			  		
					<h1 class="animate-intro">
							Writting Sample
					</h1>	
					<h1><br></h1>
					

			  	</div> 		   			
		   </div>    

		</section> 



		<!-- styles
   	================================================== -->
	   <section id="styles">
		 
			<div class="row add-bottom text-center">
				<div class="col-twelve tab-full color">
					<p class="normal">Zheming Gu
				</p><p class="normal">Professor Jingjing Hu
				</p><p class="normal">Foundation of Artificial Intelligence
				</p><p class="normal">	15 December 2018
				</p><p class="normal indent center">	Deep Learning for Real-time Strategy Video Game Artificial Intelligence
				</p><p class="normal indent">	<em class="normal">Abstract:</em> With much more actions chosen than Go, the artificial intelligence for Real-time strategy games are considered as the most challenging work today. In this article, the basic knowledge of machine learning is mentioned and possible ways to design artificial intelligence for real-time strategy games are discussed.
				</p><p class="normal indent">	<em class="normal">Index Terms:</em> artificial intelligence, real-time strategy game, machine learning, supervised learning, unsupervised learning, reinforcement learning, AlphaGo, strategy network, behavior network
				</p><p class="normal indent">	INTRODUCTION
				</p><p class="normal indent">	Real-time strategy (RTS) games such as the StarCraft by Blizzard Entertainment and Age of Empires by Ensemble Studios are world-renowned. (Buro) In the game, players build their armies to fight against each other. Design an artificial intelligence (AI) for Real-time strategy (RTS) games are challenging, because real-time planning and the long-term investment is arduous for traditional game AI design. Machine learning brings us a brand-new approach to accomplish a smarter AI for RTS games.
				</p><p class="normal indent">MACHINE LEARNING BASICS
				</p><p class="normal indent"><em class="normal">Machine learning (ML)</em> is now one of the hottest topics in computer science, with broad usage in game theory, pattern recognizing, machine vision and so on. When it comes to the learning methods of ML, it can be mainly divided into supervised learning, unsupervised learning and reinforcement learning. These three types all have their own distinct advantages. In this section, I listed a brief overview of these machine learning (ML) approaches. I, in addition, an example of AlphaGo’s architecture. in using these learning manners to achieve an astounding feat in surpass human in Go.
				</p><p class="normal indent"><em class="normal">Supervised Learning.</em> In supervised learning (SL), the task of the algorithm is to optimize an inferred function with supervised training data. The supervised training data consists of numerous supervised training examples. For each training examples, an input which traditionally is a vector and an output which is expected by the human composite a data pair. The data pair is required to feed the neural network (NN) agent. During the training, the agent receives the data pair. Input data goes through the NN then output an inferring decision. The agent exam the difference between the decision and the expected output and revises the NN model with the difference to make a better decision next time. After the training, the NN model could be an optimized inferred function to predict the relatively reasonable answer. (Resources.Saylor.Org, 2019) (Justesen)
				</p><p class="normal indent">		The NN in SL could be roughly divided into two types: feedforward neural network which core building block are convolutional layers called convolutional neural network (CNN) (Cs231n.Github.Io) and recurrent neural network (RNN). CNN is mostly applied to process images, especially image classification and object recognition. Large and deep CNN model could provide a comparatively reliable result of image classification, while a large data set should be provided in order to train this CNN model. (Krizhevsky) RNN is to learn from the sequences which include video, musical and natural language translation. Just same as CNN, RNN needs large manual data set to train the model. (Lipton)
				</p><p class="normal indent">	<em class="normal">Unsupervised Learning.</em> Unsupervised Learning (UL) is to group the data by discovering the pattern in it. Unlike SL which needs to map the input and the output data manually, the common method of UL is cluster analysis including Hierarchical clustering, k-Means clustering, Gaussian mixture models, Self-organizing maps and Hidden Markov models. (Mathworks.Com) Cluster analysis is based on the unlabeled data input, analyzing the similarity of the data and compress it to reducing the dimension while maintaining its useful information, then cluster the data which is similar into a group. 
				</p><p class="normal indent">		By using UL, patterns can be divided and the data can be preprocessed. After UL the data can be manually tagged much easier than start from scratch, then feed the data into SL to optimize an inferred function to get reasonable predicted answers.
				</p><p class="normal indent">	<em class="normal">Reinforcement Learning.</em> The paramount problem of reinforcement learning (RL) is about to solve is the Markov Decision Process (MDP). During the training, an agent interacts with the environment in discrete steps, at the same time an observer continues to monitor the environment. In each step, a state is defined, in this state the agent receives reward data from the observer, then the agent chooses an action and sent it to the environment, after that, the whole system moves into next step to finish the same job. (Mnih)
				</p><p class="normal indent">		With RL, the agent could learn to get a maximized the reward in a long-term automated. Compared with SL which learning is controlled by the human, RL is more like the machine learning itself, to discover the unknown environment by exploring it again and again. However, RL may overfit the environment it learns, even the slight change of the environment may cause the agent invalid.
				</p><p class="normal indent">	<em class="normal">AlphaGo.</em> Go has always been one of the most challenging games in the world. 10¹⁷⁰ possible board positions and obstruction of evaluating the board position make traditional AI cannot compete with humans. 
				</p><p class="normal indent">	Based on the Monte Carlo tree search (MCTS), AlphaGo uses a value network and policy network to evaluate each step. (Silver) Policy network is trained by SL and RL, during MCTS’s expansion, the policy network would predict a prior probability, which stands for evaluation of the chess position. The value network is trained by RL, during MCTS’s simulation, the value network would infer the action values, which update the value of nodes to its parent node until the root node of Monte Carlo tree. The value of each action consists of prior probability and action value to fully utilize the advantage of the policy network and the value network.
				</p><p class="normal indent">	DEEP LEARNING FOR RTS GAME AI DESIGN 
				</p><p class="normal indent">	The goal of RTS games is to defeat the enemy. With more resources, the possibility of winning the game would become higher. Hence, in each step, the AI needs to maximize the resources and minimize the enemy’s resource. The progress of the game can be considered as the MDP, so the RL would be helpful in this case. 
				</p><p class="normal indent">		At the same time, the SL policy network in AlphaGo provides better performance in Go playing (Silver). SL can generate a model to mimic human strategy. In practice, imitate human strategy have its own advantages which have been demonstrated by the AlphaGo, thus SL also needs to be put into the list of RTS game AI design.
				</p><p class="normal indent">	The RTS game has lots of commonplace with Go. They both have numerous legal action choice in each state, they are all tasked to evaluate the “board position” to gain a better situation. The method AlphaGo used could also be helpful in building AI for RTS games. There could be two kinds of NN to play the game. One of the networks could be larger to achieve better accuracy of evaluating the current situation which called the strategy network. The other is smaller to manipulate the game characters in real-time to react with the environment, which called behavior network.
				</p><p class="normal indent">		For the first training stage, the strategy network would be trained in predicting expert actions using SL. Taking the current number and state of each kind of soldier and building as input, the final layer outputs a suitable action for game characters to execute. The main task for the strategy network is to control the game in the macroscopic range. Thus, the action here is to instruct a specific army to attack a particular position or mission ad hoc worker unit to build resources. The second stage aims at improving the strategy network by RL. Inherence the weight of convolutional layers of SL strategy network to RL strategy network, then play games between the current RL strategy network with randomly selected previous strategy network to prevent overfitting. 
				</p><p class="normal indent">		Behavior network controls the microscopic range operation of each army. Based on the instruction from the strategy network, lightweight network architecture improve the reaction of the army to the environment. So, each army can focus on the environment state change, observing the enemy’s motion, then decide the next action quickly. In this step of training pipeline, RL would be used to train the network to gain a relatively long-term reward for each army.
				</p><p class="normal indent">		CONCLUSION
				</p><p class="normal indent">	Machine learning has a huge potential in building game AI, especially in RTS games. Using deep learning methods in RTS game AI design could achieve a smarter AI than the traditional one. The architecture of Alpha Go inspired me that ML methods shouldn’t be isolated. Different ML methods have their own distinct advantages, if we combine them together in a training process, the superiority of them could be fully used.
 
				</p><p class="normal indent center">	Works Cited
				</p><p class="normal indent center">	Buro, Michael. "Real-time strategy games: A new AI research challenge." IJCAI. 2003.
				</p><p class="normal indent center">	Resources.Saylor.Org, 2019, https://resources.saylor.org/wwwresources/archived/site/wp-    content/uploads/2011/11/CS405-6.2.1.2-WIKIPEDIA.pdf. 
				</p><p class="normal indent center">	Justesen, Niels, et al. "Deep learning for video game playing." arXiv preprint arXiv:1708.07902 (2017). 
				</p><p class="normal indent center">	Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep     convolutional neural networks." Advances in neural information processing systems. 2012. 
				</p><p class="normal indent center">	"Cs231n Convolutional Neural Networks For Visual Recognition". Cs231n.Github.Io, 2019,     http://cs231n.github.io/convolutional-networks/. 
				</p><p class="normal indent center">	Lipton, Zachary C., John Berkowitz, and Charles Elkan. "A critical review of recurrent neural networks for sequence learning." arXiv preprint arXiv:1506.00019 (2015). 
				</p><p class="normal indent center">	"Unsupervised Learning". Mathworks.Com, 2019,     https://www.mathworks.com/discovery/unsupervised-learning.html. 
				</p><p class="normal indent center">	Mnih, Volodymyr, et al. "Asynchronous methods for deep reinforcement learning." International conference on machine learning. 2016. 
				</p><p class="normal indent center">Silver, David, et al. "Mastering the game of Go with deep neural networks and tree search." nature 529.7587 (2016): 484.
							
	
						
							
					</p>
				</div>
			</div>
		
			
	
			<div class="row">
				<a class="button button-primary pull-right " href="../../index.html">Back Home ></a>
			</div>		
		
		   </section>	
   
   </div> <!-- /main-content-wrap -->


   <!-- footer
	================================================== -->
	<footer id="main-footer">

		
	 
			<div class="footer-info-wrap">
	 
				<div class="row footer-info">
	 
					   <div class="col-four tab-full">
						   <h4><i class="icon-location-map-1"></i> Where to Find Me</h4>
	 
						   <p>
					  The People's Bank of China<br>
					  No.331, Fuhui road, Gucheng district<br>
					  Lijiang city, Yunnan province, China 674100
					  </p>
					   </div>
	 
					<div class="col-four tab-full collapse">
						<h4><i class="icon-phone-incoming"></i> Get In Touch</h4>
	 
						<p>zheminggugu@gmail.com<br>
							Wechat: gzm1637120736 <br>
							Phone: (+86) 18811378006			     
						</p>
					</div>
	 
					<div class="col-four tab-full">
						<h4><i class="icon-organization-hierarchy-3"></i> Recommonders</h4>
	 
						<ul class="footer-link-list">
							<li><a href="#">Gangyi Ding</a></li>
							<li><a href="#">Hongsong Li</a></li>
							<li><a href="#">Li Lin</a></li>
						</ul>
					</div>
						
				   </div>
			</div> <!-- /footer-info-wrap -->
				
			<div class="footer-bottom"> 
	 
				<div class="copyright">
					  <span>This website is just for study use.</span> 
					  <span>Templete design by <a href="#">Elevate</a></span>	         	
				</div>  		
			</div>
				
		</footer>   

   <div id="go-top">
		<a class="smoothscroll" title="Back to Top" href="#top"><i class="fa fa-long-arrow-up"></i></a>
	</div>

   <!-- preloader
   ================================================== -->
   <div id="preloader"> 
    	<div id="loader"></div>
   </div> 

   <!-- Java Script
   ================================================== --> 
   <script src="../../js/jquery-2.1.3.min.js"></script>
   <script src="../../js/plugins.js"></script>
   <script src="../../js/main.js"></script>

</body>

</html>